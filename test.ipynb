{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 0.409\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross entropy: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ce(logits, target))\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m  ce(logits, target)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    " \n",
    "logits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\n",
    "target = torch.tensor([[0., 1., 0.], [1., 0., 0.]])\n",
    "print(\"Cross entropy: %.3f\" % ce(logits, target))\n",
    "loss =  ce(logits, target)\n",
    "ce(logits, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1513e-03, -1.2605e-02,  1.0000e+00, -7.8782e-04, -7.8782e-04],\n",
      "        [-3.1653e-02, -3.5170e-03, -3.5170e-03, -1.4068e-02,  1.0000e+00],\n",
      "        [ 1.0000e+00, -1.0285e-03, -4.1138e-03, -6.5821e-02, -6.5821e-02]])\n",
      "Label Smoothing Loss: 1.4038673639297485\n"
     ]
    }
   ],
   "source": [
    "class LabelSmoothingLoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_smoothing = smoothing\n",
    "\n",
    "    def forward(self, output, target):\n",
    " # Convert target labels to one-hot encoding\n",
    "        target_one_hot = torch.zeros_like(output)\n",
    "        target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
    "        print(target_smoothed)\n",
    "        # Compute cross-entropy loss\n",
    "        loss = -(torch.nn.functional.log_softmax(output, dim=1) * target_smoothed).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# Example usage\n",
    "num_classes = 5\n",
    "smoothing = 0.3\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing)\n",
    "\n",
    "# Example true labels\n",
    "true_labels = torch.tensor([2, 4, 0])\n",
    "\n",
    "# Example predicted probabilities\n",
    "pred_probs = torch.tensor([[0.1, 0.2, 0.6, 0.05, 0.05],\n",
    "                           [0.3, 0.1, 0.1, 0.2, 0.3],\n",
    "                           [0.05, 0.05, 0.1, 0.4, 0.4]])\n",
    "\n",
    "loss = criterion(pred_probs, true_labels)\n",
    "print(\"Label Smoothing Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Cross entropy built in: 1.8174\n",
      "tensor([[0.8667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.8667, 0.0667]])\n",
      "tensor(1.8174)\n",
      "Cross entropy: 1.8174\n",
      "tensor([[0.8667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.8667, 0.0667]])\n",
      "tensor(1.8174)\n",
      "Cross entropy: 1.8981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)\n",
    "ce = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "loss = LabelSmoothingLoss(3, 0.2)\n",
    "\n",
    "logits = torch.tensor([[-1.90, -0.29, -2.30], [-0.29, -1.90, -2.30]])\n",
    "target = torch.tensor([[1.0, 0., 0.], [0., 1., 0.]])\n",
    "target2 = torch.tensor([0, 1])\n",
    "\n",
    "print(\"Cross entropy built in: %.4f\" % ce(logits, target2))\n",
    "print( loss(logits, target2))\n",
    "\n",
    "print(\"Cross entropy: %.4f\" % ce(logits, target2))\n",
    "\n",
    "print(loss(logits, target2))\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Cross entropy: %.4f\" % ce(logits, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2294, -0.7213, -1.1914,  0.1214, -1.2865],\n",
      "        [ 0.5542, -1.3177, -0.3021,  1.6507, -1.0291],\n",
      "        [-0.4412, -0.5488, -0.7729, -1.1762,  1.2069]], requires_grad=True)\n",
      "tensor([[0.0920, 0.2227, 0.2135, 0.2961, 0.1756],\n",
      "        [0.0368, 0.0719, 0.5036, 0.1418, 0.2459],\n",
      "        [0.2943, 0.1632, 0.2078, 0.2913, 0.0435]])\n",
      "tensor(2.3083, grad_fn=<DivBackward1>)\n",
      "tensor(1.8266, grad_fn=<DivBackward1>)\n",
      "tensor(1.1471, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "\n",
    "print(input)\n",
    "print(target)\n",
    "output = ce(input, target)\n",
    "print(output)\n",
    "\n",
    "t1 = torch.tensor([[ 0.5161,  0.6198, -1.4475,  0.4004, -0.3914],\n",
    "        [ 0.9718,  0.4976,  0.6293, -1.1079, -0.3110],\n",
    "        [ 1.2641,  0.9328, -1.8087,  0.1155, -0.3138]], requires_grad=True)\n",
    "t2 = torch.tensor([[0.5983, 0.0077, 0.3152, 0.0430, 0.0358],\n",
    "        [0.2199, 0.2125, 0.4064, 0.0695, 0.0919],\n",
    "        [0.4249, 0.1048, 0.3183, 0.0697, 0.0824]])\n",
    "\n",
    "output = ce(t1, t2)\n",
    "print(output)\n",
    "t2single = torch.tensor([0, 2, 0])\n",
    "output = ce(t1, t2single)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Taken from Ordinal Classification Paper provided\n",
    "import math \n",
    "\n",
    "def soft_encoded_vector(label, penalty):\n",
    "    \"\"\"\n",
    "    Function to encode Label into Soft Encoded Vector\n",
    "    inputs:\n",
    "    label (int): Ground Truth Label//.lkl;'\n",
    "    \n",
    "     \n",
    "      \n",
    "        (float): Penalty for Labels further from Ground Truth\n",
    "    \n",
    "    outputs:\n",
    "    soft_vector (list of Float): Soft Encoded Vector of Ground Truth\n",
    "    \n",
    "    \"\"\"\n",
    "    classes = [0,1,2]\n",
    "    \n",
    "    cost = lambda val, label : penalty*(abs(label-val))\n",
    "    expr = lambda val, label : math.exp(-cost(val, label))\n",
    "    \n",
    "    total_sum = sum([expr(val,label) for val in classes])\n",
    "    \n",
    "    soft_vector = [(expr(val,label)/total_sum) for val in classes]\n",
    "    \n",
    "    return (soft_vector)\n",
    "\n",
    "\n",
    "def convert_labels_to_vec(labels,penalty):\n",
    "    \"\"\"\n",
    "    Function to take list of Labels & convert all to list of soft encodings\n",
    "    inputs:\n",
    "    labels (list of Int): List of Labels for batch\n",
    "    penalty (float): Penalty for Labels further from Ground Truth\n",
    "    \n",
    "    outputs:\n",
    "    2d Array (list of list of Float): List of Soft Encoded Vectors\n",
    "    \n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    for label in labels:\n",
    "        outputs.append(soft_encoded_vector(label,penalty))\n",
    "    \n",
    "    return torch.Tensor(outputs)\n",
    "\n",
    "# Example of Usage + Output\n",
    "example_labels = torch.Tensor([2])\n",
    "print(convert_labels_to_vec(example_labels,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = original_labels * (1 - label_smoothing) + label_smoothing / num_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
